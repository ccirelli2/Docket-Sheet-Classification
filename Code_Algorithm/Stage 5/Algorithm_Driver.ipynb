{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''03.31.2018\n",
    "\n",
    "\n",
    "Next Steps:\n",
    "- Test the Step3A code to see if it runs with the new names in the name space. \n",
    "- Test the Step3B code. same. \n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORT MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(r'/home/ccirelli2/Desktop/Docket-Sheet-Classification/Modules')\n",
    "import Step1_Module_Ngrams_FreqDist_version4_Ngrams as stp1\n",
    "import Step2_P2_Module_Get_Top_Words_version4_Ngrams as stp2\n",
    "import Step3_Module_Ngrams_Docketsheet_KeyWord_FreqDist as stp3\n",
    "import Step4_Module_Machine_Learning_Algorithms as stp4\n",
    "import Step5_Module_Measure_DocketSheetEntries_NoMatches as stp5_nomatch\n",
    "import Step5_Module_Measure_Dependencies_Between_Stages as stp5_dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORT NOGRAM RESULTS:  CALCULATION = AVERAGE APPEARANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(r'/home/ccirelli2/Desktop/Docket-Sheet-Classification/All_Results/Result_Ngrams')\n",
    "Nograms_appearance = pd.read_excel(r'Docketsheet_FreqDist_Nograms_Average_appearance.xlsx')\n",
    "Bigrams_appearance = pd.read_excel(r'Docketsheet_FreqDist_Bigrams_Average_appearance.xlsx')\n",
    "Trigrams_appearance = pd.read_excel(r'Docketsheet_FreqDist_Trigrams_Average_appearance.xlsx')\n",
    "Quadgrams_appearance = pd.read_excel(r'Docketsheet_FreqDist_Quadgrams_Average_appearance.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORT NOGRAM RESULTS:  CALCULATION = FREQUENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Nograms_frequency = pd.read_excel(r'Docketsheet_FreqDist_Nograms_Frequency_distribution.xlsx')\n",
    "Bigrams_frequency = pd.read_excel(r'Docketsheet_FreqDist_Bigrams_Frequency_distribution.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORT DOCKETSHEET WITH PRE-CLASSIFIED STAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'/home/ccirelli2/Desktop/Docket-Sheet-Classification/Data_Files_applicable_all_code')\n",
    "Docketsheet_train = 'DocketSheet Classification_70_02.22.2018.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORT MASTER DOCKETSHEET WITH NON-CLASSIFIED STAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if the document has yet to be created\n",
    "'''\n",
    "File_masterdocketsheet = r'Master_Docket_sheets.xlsx'\n",
    "Destination_loc = r'/home/ccirelli2/Desktop/Docket-Sheet-Classification/Data_Files_applicable_all_code'\n",
    "df_transformed = stp3.transform_Master_Docketsheet(File_masterdocketsheet, Write2Excel =True, \n",
    "                                                  destination_location = Destination_loc)\n",
    "'''\n",
    "Docketsheet_test = r'Master_Docketsheet_transformed.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DOCUMENTATION MASTER DRIVER FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"DOCUMENTATION\\n    \\n    STEP2 - IDENTIFYING KEY NGRAMS\\n    Inputs          i.) FreqDist_File - file where our frequency distributions are saved for each ngramtype.\\n     ii.) Calculation Method - see below; iii.) Methodology_top_words - see below; \\n     iv.) Write2excel  -self explanatory; v.) destination_location - where to write file; \\n     vi.) Ngramtype - tell algorithm which ngram type we are dealing with.  \\n    Operations      (Basic Explanation) Based on the combinationof Calculation + Methodology, the algorithm\\n     will chose the KeyNgrams most unique (or least) to a given Stage. \\n     \\n    Methodology Calculation of Central Tendencys\\n     Options     'CalculationI_homebrew_STDV', \\n                 'CalculationII_AVG_not_zero', \\n                 'CalculationIII_Correlation_Coefficient'\\n    Methodology Top Words\\n     Options     'Top15_highest_STDV', \\n                 'Top15_highest_COCOEF', \\n                 'Top5_highest_STDV_lowest_AVG', \\n                 'Top5_highest_STDV_AVG_below_20prct', \\n                 'Top5_lowest_STDV_highest_AVG', \\n                 'Top5_lowest_COCOEF_highest_AVG'\\n    \\n    STEP3 - GET KEY NGRAMS IN DOCKETSHEETS\\n    Inputs         i.)   The original docketsheet with preclassified time periods (stages)\\n    ii.)  The KeyNgrams generated from Step2. \\n    Output         iii.) A dataframe with the appearance of the KeyNgrams for each docket sheet entry. \\n    \\n    STEP4 - MACHINE LEARNING ALGORITHMS\\n    Input:         i.)   Target_dir  = location where our docketsheet key word appearance dataframes are located. \\n    ii.)  Depth       = the depth that we want to use for our tree.  If not specified default \\n                      to 8. \\n    iii.) Write2Excel = if we want to write to Excel or work with the results in memory. \\n                      this feature is not yet set up for the confusion matrix or class report. \\n    iv.)  Destination = where we want to write our results to. \\n    v.)   Iterable    = whether we are working with a single or multiple files. \\n    vi.)  Single_file = if we chose False for the Iterable, then we will need to specify the \\n                      file we want to use. \\n vii.)    Metric      = the metric that we want to use to guage the performance of our model. \\n                      Options Include (must be string):\\n                          a.) Accuracy (default): The precision of our model. \\n                          b.) Matrix:  Return the confusion matrix, \\n                          c.) Report: to return  the classificaiton report, \\n                          d.) Export_Indv_Predictions: returns a dataframe object containing the predictions.\\n viii.)   KeyWord     = Choose the key word that you want to use to group the files (approachs)\\n                      to be used in the ML model. Examples include using the names of the \\n                      ngrmas ('Bigrams') or it could be STDV vs COCOEF, etc. \\n    Operations i.)     The main operation here is either to iterate a list of files in a directory to \\n        generate the predictions or to work with one file.  That and the code is set up so \\n        that the user can have various choices as can be inferred from the input explanations. \\n    \\n    \\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''DOCUMENTATION\n",
    "    \n",
    "    STEP1 - CREATING NGRAM FREQUENCY OR APPEARANCE DISTRIBUTIONS \n",
    "    Comments        This step has already been completed using the \n",
    "                    'Step_1_Code_DocketSheet_Ngrams_FreqDist_by_Life Cycle Classification' code.  Seeing that\n",
    "                    these objects only need to be created once, there was no need to integrate them into the\n",
    "                    master driver function.  Rather they have been created and are imported in the beginning\n",
    "                    of this file. \n",
    "    \n",
    "    STEP2 - IDENTIFYING KEY NGRAMS\n",
    "    Inputs          i.) FreqDist_File - file where our frequency distributions are saved for each ngramtype.\n",
    "                    ii.) Calculation Method - see below; iii.) Methodology_top_words - see below; \n",
    "                    iv.) Write2excel  -self explanatory; v.) destination_location - where to write file; \n",
    "                    vi.) Ngramtype - tell algorithm which ngram type we are dealing with.  \n",
    "    Operations      (Basic Explanation) Based on the combinationof Calculation + Methodology, the algorithm\n",
    "                    will chose the KeyNgrams most unique (or least) to a given Stage. \n",
    "                    \n",
    "    Methodology Calculation of Central Tendencys\n",
    "                    Options     'CalculationI_homebrew_STDV', \n",
    "                                'CalculationII_AVG_not_zero', \n",
    "                                'CalculationIII_Correlation_Coefficient'\n",
    "    Methodology Top Words\n",
    "                    Options     'Top15_highest_STDV', \n",
    "                                'Top15_highest_COCOEF', \n",
    "                                'Top5_highest_STDV_lowest_AVG', \n",
    "                                'Top5_highest_STDV_AVG_below_20prct', \n",
    "                                'Top5_lowest_STDV_highest_AVG', \n",
    "                                'Top5_lowest_COCOEF_highest_AVG'\n",
    "    \n",
    "    STEP3 - GET KEY NGRAMS IN DOCKETSHEETS\n",
    "    \n",
    "    PartI: Docketsheet - Train\n",
    "    Inputs         i.)   The original docketsheet with preclassified time periods (stages)\n",
    "                   ii.)  The KeyNgrams generated from Step2. \n",
    "    Output         iii.) A dataframe with the appearance of the KeyNgrams for each docket sheet entry. \n",
    "    \n",
    "    PartII: Docketsheet - Test\n",
    "    Inputs         i.)   The docketsheet that includes all of the docketsheet entries, not just those that \n",
    "                         were pre-classified. \n",
    "    Operations     ii.)  We need to adjust the docketsheet to match the structure (col/row) that was used \n",
    "                         when creating the Step3 Module. \n",
    "                   iii.) \n",
    "    \n",
    "    STEP4 - MACHINE LEARNING ALGORITHMS\n",
    "    Input:         i.)   Target_dir  = location where our docketsheet key word appearance dataframes are located. \n",
    "                   ii.)  Depth       = the depth that we want to use for our tree.  If not specified default \n",
    "                                     to 8. \n",
    "                   iii.) Write2Excel = if we want to write to Excel or work with the results in memory. \n",
    "                                     this feature is not yet set up for the confusion matrix or class report. \n",
    "                   iv.)  Destination = where we want to write our results to. \n",
    "                   v.)   Iterable    = whether we are working with a single or multiple files. \n",
    "                   vi.)  Single_file = if we chose False for the Iterable, then we will need to specify the \n",
    "                                     file we want to use. \n",
    "                vii.)    Metric      = the metric that we want to use to guage the performance of our model. \n",
    "                                     Options Include (must be string):\n",
    "                                         a.) Accuracy (default): The precision of our model. \n",
    "                                         b.) Matrix:  Return the confusion matrix, \n",
    "                                         c.) Report: to return  the classificaiton report, \n",
    "                                         d.) Export_Indv_Predictions: returns a dataframe object containing the predictions.\n",
    "                viii.)   KeyWord     = Choose the key word that you want to use to group the files (approachs)\n",
    "                                     to be used in the ML model. Examples include using the names of the \n",
    "                                     ngrmas ('Bigrams') or it could be STDV vs COCOEF, etc. \n",
    "    Operations i.)     The main operation here is either to iterate a list of files in a directory to \n",
    "                       generate the predictions or to work with one file.  That and the code is set up so \n",
    "                       that the user can have various choices as can be inferred from the input explanations. \n",
    "\n",
    "'''   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MasterFunction(stp2_FreqDist_file, \n",
    "                        stp2_Calculation_meth, stp2_methodology_top_words, \n",
    "                        stp2_write2excel, stp2_destination_location, \n",
    "                        stp2_Ngram_type,\n",
    "                   \n",
    "                   stp3A_Docketsheet, \n",
    "                        stp3A_DirNgramLoc, stp3A_Iterable, \n",
    "                        stp3A_KeyPhrase, stp3A_Destination_location,\n",
    "                        stp3A_Transpose4mlModel, stp3A_Write2Excel,\n",
    "                   \n",
    "                   stp3B_Docketsheet, \n",
    "                        stp3B_DirNgramLoc, stp3B_Iterable, \n",
    "                        stp3B_KeyPhrase, stp3B_Destination_location,\n",
    "                        stp3B_Transpose4mlModel, stp3B_Write2Excel,\n",
    "                   \n",
    "                   stp4_Max_Depth, \n",
    "                        stp4_TrainTest, \n",
    "                        stp4_Metric):\n",
    "    \n",
    "    # Step2: Get Key Ngrams From Frequency Distributions\n",
    "    print('Step2: Generating our key Ngrmas...\\n')\n",
    "    TopWords_Toggle_Calc_Methodology           = stp2.get_top_words_toggle_methodology(\n",
    "                    # Features\n",
    "                    stp2_FreqDist_file, \n",
    "                    stp2_Calculation_meth, \n",
    "                    stp2_methodology_top_words, \n",
    "                    stp2_write2excel, \n",
    "                    stp2_destination_location, \n",
    "                    stp2_Ngram_type)\n",
    "    \n",
    "    # If writing to Excel, revert back to main dir where docketsheet is located. \n",
    "    os.chdir(r'/home/ccirelli2/Desktop/Docket-Sheet-Classification/Data_Files_applicable_all_code')\n",
    "     \n",
    "    # Step3_A: Get Key Ngram Appearance For Training Docketsheet Entries\n",
    "    print('Step3_A: Generating the Ngram matches for the \\'Training\\' Docketsheet entries...\\n')\n",
    "    KeyNgram_Appearance_Training_DocketSheetEntries = stp3.get_DocketSheet_KeyWord_Appearance_Master(\n",
    "                    # Features\n",
    "                    stp3_Docketsheet                 = stp3A_Docketsheet, \n",
    "                    stp3_DirNgramLoc                 = stp3A_DirNgramLoc, \n",
    "                    stp3_Iterable                    = stp3A_Iterable, \n",
    "                    stp3_KeyPhrase                   = stp3A_KeyPhrase, \n",
    "                    stp3_Destination_location        = stp3A_Destination_location,\n",
    "                    stp3_Transpose4mlModel           = stp3A_Transpose4mlModel, \n",
    "                    stp3_Write2Excel                 = stp3A_Write2Excel, \n",
    "                    stp3_Target_file                 = TopWords_Toggle_Calc_Methodology)\n",
    "    \n",
    "    # Step3_B: Get Key Ngram Appearance For Test Docketsheet Entries\n",
    "    # Amendment 03.31.2018:  Step_3 B was added. \n",
    "    print('Step3_B: Generating the Ngram matches for the \\'Test\\' Docketsheet entries...\\n')\n",
    "    KeyNgram_Appearance_Test_DocketSheetEntries = stp3.get_DocketSheet_KeyWord_Appearance_Master(\n",
    "                    # Features\n",
    "                    stp3_Docketsheet                 = stp3B_Docketsheet, \n",
    "                    stp3_DirNgramLoc                 = stp3B_DirNgramLoc, \n",
    "                    stp3_Iterable                    = stp3B_Iterable, \n",
    "                    stp3_KeyPhrase                   = stp3B_KeyPhrase, \n",
    "                    stp3_Destination_location        = stp3B_Destination_location,\n",
    "                    stp3_Transpose4mlModel           = stp3B_Transpose4mlModel, \n",
    "                    stp3_Write2Excel                 = stp3B_Write2Excel, \n",
    "                    stp3_Target_file                 = TopWords_Toggle_Calc_Methodology)\n",
    "    \n",
    "    # Step4: Decision Tree Prediction\n",
    "    print('Step4:  Generating Decision Tree prediction\\n')\n",
    "    DecisionTreePrediction = stp4.simple_decision_tree(\n",
    "                    stp4_Max_Depth                        = stp4_Max_Depth, \n",
    "                    stp4_TrainTest                        = stp4_TrainTest, \n",
    "                    stp4_Metric                           = stp4_Metric,\n",
    "                    stp4_Docketsheet_wordFreqFile         = KeyNgram_Appearance_Training_DocketSheetEntries)\n",
    "       \n",
    "    # Define File Name for In-Memory Reference (Not reading from a file + path name)\n",
    "    df_inmemory_name = str(stp2_Ngram_type + '_' + stp2_Calculation_meth + '_' + \n",
    "    stp2_methodology_top_words + '_' + stp4_Metric + '_' + 'Depth' + '_' + str(stp4_Max_Depth))\n",
    "    \n",
    "    # Calculate the number of docket sheet rows without a match.  Send to user via print. \n",
    "    Count_zero = stp5_nomatch.get_count_nomatch_columns(\n",
    "        File = KeyNgram_Appearance_DocketSheetEntries, Count = 'Count_zero')\n",
    "    Count_total = stp5_nomatch.get_count_nomatch_columns(\n",
    "        File = KeyNgram_Appearance_DocketSheetEntries, Count = 'Count_all')\n",
    "    \n",
    "    # Calculate the extent of dependencies between the KeyWords in Each Stage. \n",
    "    Perct_dependency = stp5_dependencies.measure_dependencies(TopWords_Toggle_Calc_Methodology)\n",
    "    \n",
    "    # OTHER PERFORMANCE METRICS REGARDING THE MODEL\n",
    "    \n",
    "    print('Other performance metrics:')\n",
    "    print('\\tPercent Dependency:   The Ngram Key Selection resulted in =>' + ' ' + str(Perct_dependency) + ' of overlap (dependency) between the key words')\n",
    "    print('\\tPercent None-Matches: In addition, this selection resulted in => ', round(Count_zero/Count_total, 2)*100, ' ' + \n",
    "          'percentage of Docketsheet rows without a matching Ngram', '\\n')\n",
    "\n",
    "    return DecisionTreePrediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNote that you the ytrain and test objects likely contain the arrays and attributes that we are calling like confusion matrix \\nand or report are likely just ways to arrange these predictions. \\n\\n\\n**** Note that you created a new object in Step4, the decision tree model, that allows for you to return a dataframe of the \\nindividual predictions.  This function has dependencies, in particular the second function that generates the Accuracy, \\nMatrics, etc scores.  You need to find a way to stop the second function from running and to only pass the data frame \\nobject to the user. \\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Note that you the ytrain and test objects likely contain the arrays and attributes that we are calling like confusion matrix \n",
    "and or report are likely just ways to arrange these predictions. \n",
    "**** Note that you created a new object in Step4, the decision tree model, that allows for you to return a dataframe of the \n",
    "individual predictions.  This function has dependencies, in particular the second function that generates the Accuracy, \n",
    "Matrics, etc scores.  You need to find a way to stop the second function from running and to only pass the data frame \n",
    "object to the user. \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RUN FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step2: Generating our key Ngrmas...\n",
      "\n",
      "Step3: Generating the Ngram matches for the Docketsheet entries...\n",
      "\n",
      "Returning training accuracy score:\n",
      "\tAccuracy_score for the training set =>  0.95 \n",
      "\n",
      "Returning test accuracy score:\n",
      "\tAccuracy score for test set => 0.92 \n",
      "\n",
      "Other performance metrics:\n",
      "\tPercent Dependency:   The Ngram Key Selection resulted in => 30.0 of overlap (dependency) between the key words\n",
      "\tPercent None-Matches: In addition, this selection resulted in =>  0.0  percentage of Docketsheet rows without a matching Ngram \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccirelli2/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "KeyNgrams = MasterFunction(\n",
    "\n",
    "                    stp2_FreqDist_file             = Nograms_frequency,             # In general Freq seems to perf better. \n",
    "                        stp2_Calculation_meth      = 'CalculationI_homebrew_STDV', \n",
    "                        stp2_methodology_top_words = 'Top15_highest_STDV', \n",
    "                        stp2_write2excel           = False, \n",
    "                        stp2_destination_location  = '/home/ccirelli2/Desktop/', \n",
    "                        stp2_Ngram_type            = 'Nograms', \n",
    "\n",
    "                    stp3A_Docketsheet               = Docketsheet_train, \n",
    "                        stp3A_DirNgramLoc           = None, \n",
    "                        stp3A_Iterable              = False, \n",
    "                        stp3A_KeyPhrase             = None, \n",
    "                        stp3A_Destination_location  = None,\n",
    "                        stp3A_Transpose4mlModel     = True, \n",
    "                        stp3A_Write2Excel           = False, \n",
    "\n",
    "                    stp4_Max_Depth                  = 13, \n",
    "                        stp4_TrainTest              = 'Test', \n",
    "                        stp4_Metric                 = 'Accuracy')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stp3.write_to_excel(KeyNgrams, '/home/ccirelli2/Desktop/', 'OutputMasterFile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KeyNgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
