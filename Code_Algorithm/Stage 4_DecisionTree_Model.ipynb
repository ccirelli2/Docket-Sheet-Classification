{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import graphviz\n",
    "from sklearn.tree import export_graphviz    \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#   Classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\Chris.Cirelli\\\\Desktop\\\\Python Programming Docs\\\\GSU\\\\Sprint Project\\\\Docket-Sheet-Classification\\\\Modules')\n",
    "import Step1_Module as stp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\\\Users\\\\Chris.Cirelli\\\\Desktop\\\\Python Programming Docs\\\\GSU\\\\Sprint Project\\\\Docket-Sheet-Classification\\\\Result_Files_Key_Word_Attempt_2')\n",
    "\n",
    "df_DAT = pd.read_excel(r'C:\\Users\\Chris.Cirelli\\Desktop\\Python Programming Docs\\GSU\\Sprint Project\\Docket-Sheet-Classification\\Result_Files_Key_Word_Attempt_2\\Data Analytics Table_WordMatch_All_Docket_Entries_v2.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Index (Docket Sheet Entries)\n",
    "\n",
    "df_reset_index = df_DAT.reset_index()\n",
    "df_drop_col1 = df_reset_index.drop('index', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = df_drop_col1.drop('Life Cycle Stage', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = df_drop_col1['Life Cycle Stage']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Simple Decision Tree\n",
    "\n",
    "def simple_decision_tree(Features, Targets):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Features, Targets, test_size=0.2)\n",
    "    clf = DecisionTreeClassifier(max_depth = 35, random_state = 50)\n",
    "    \n",
    "    # Fit Algorithm\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    #   Train\n",
    "    clf_pred = clf.predict(X_train)\n",
    "    report_train = sklearn.metrics.classification_report(y_train, clf_pred)\n",
    "    matrix_train = sklearn.metrics.confusion_matrix(y_train, clf_pred)\n",
    "    \n",
    "    #   Test\n",
    "    clf_pred = clf.predict(X_test)\n",
    "    report_test = sklearn.metrics.classification_report(y_test, clf_pred)\n",
    "    matrix_test = sklearn.metrics.confusion_matrix(y_test, clf_pred)\n",
    "    \n",
    "    print('Report (Train)', '\\n', report_train, '\\n')\n",
    "    print('Report (Test)', '\\n', report_test, '\\n')\n",
    "    print('Confusion Matrix (Test)', '\\n', matrix_test, '\\n')\n",
    "    \n",
    "    return matrix_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report (Train) \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00        87\n",
      "          2       0.98      1.00      0.99        96\n",
      "          3       0.98      0.98      0.98        81\n",
      "          4       0.86      0.95      0.90        44\n",
      "          5       1.00      1.00      1.00       471\n",
      "          6       1.00      1.00      1.00        59\n",
      "          7       1.00      0.96      0.98        57\n",
      "          8       0.95      0.89      0.92        63\n",
      "         10       1.00      1.00      1.00        33\n",
      "         11       1.00      1.00      1.00        45\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1036\n",
      " \n",
      "\n",
      "Report (Test) \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.78      1.00      0.88        21\n",
      "          2       0.95      0.91      0.93        23\n",
      "          3       0.82      0.78      0.80        18\n",
      "          4       0.42      0.57      0.48        14\n",
      "          5       0.97      0.95      0.96       123\n",
      "          6       0.71      0.86      0.77        14\n",
      "          7       0.83      0.59      0.69        17\n",
      "          8       0.40      0.38      0.39        16\n",
      "          9       0.00      0.00      0.00         2\n",
      "         10       1.00      1.00      1.00         4\n",
      "         11       1.00      0.86      0.92         7\n",
      "\n",
      "avg / total       0.85      0.85      0.84       259\n",
      " \n",
      "\n",
      "Confusion Matrix (Test) \n",
      " [[ 21   0   0   0   0   0   0   0   0   0   0]\n",
      " [  1  21   0   0   1   0   0   0   0   0   0]\n",
      " [  0   0  14   3   0   0   0   1   0   0   0]\n",
      " [  2   0   0   8   0   0   0   4   0   0   0]\n",
      " [  1   0   0   0 117   2   1   2   0   0   0]\n",
      " [  0   0   0   0   1  12   0   1   0   0   0]\n",
      " [  1   1   0   4   0   1  10   0   0   0   0]\n",
      " [  1   0   3   4   1   0   1   6   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   4   0]\n",
      " [  0   0   0   0   0   0   0   1   0   0   6]] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris.Cirelli\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "test = simple_decision_tree(df_feature, df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21  0  0  0  0  0  0  0  0  0  0]\n",
      "[ 1 21  0  0  1  0  0  0  0  0  0]\n",
      "[ 0  0 14  3  0  0  0  1  0  0  0]\n",
      "[2 0 0 8 0 0 0 4 0 0 0]\n",
      "[  1   0   0   0 117   2   1   2   0   0   0]\n",
      "[ 0  0  0  0  1 12  0  1  0  0  0]\n",
      "[ 1  1  0  4  0  1 10  0  0  0  0]\n",
      "[1 0 3 4 1 0 1 6 0 0 0]\n",
      "[0 0 0 0 0 2 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 4 0]\n",
      "[0 0 0 0 0 0 0 1 0 0 6]\n"
     ]
    }
   ],
   "source": [
    "for x in test:\n",
    "    print(x)saz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Tree Using GridSearchCV\n",
    "def tree_I_GridSearchCV(Features, Targets):\n",
    "    x_train, X_test, y_train, Y_test = train_test_split(Features, Targets, \n",
    "                                                        test_size=0.3, \n",
    "                                                        random_state = 50, \n",
    "                                                        stratify = Targets)\n",
    "    param_grid = {'max_depth': [20, 30, 40, 50], \n",
    "#                   'min_samples_split':[25,30,35], \n",
    "#                   'min_samples_leaf': [25,30,35], \n",
    "#                   'min_weight_fraction_leaf': [.01, .025, .05], \n",
    "#                   'max_features':[2,3,4], \n",
    "#                   'max_leaf_nodes':[15,20,25], \n",
    "                 }\n",
    "    tree = DecisionTreeClassifier()\n",
    "    grid_search = GridSearchCV(tree, param_grid, scoring = 'accuracy')\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    \n",
    "    # Train Prediction\n",
    "    prediction = grid_search.predict(x_train)\n",
    "    best_params = grid_search.best_params_\n",
    "    # Train Results\n",
    "    class_report = sklearn.metrics.classification_report(X_test, prediction)\n",
    "    print('Best Parameters (train) =>', best_params, '\\n','\\n')\n",
    "    print('Train Classification Report (train): ', '\\n', class_report)\n",
    "    \n",
    "#     # Test Prediction\n",
    "#     prediction = grid_search.predict(y_train)\n",
    "#     best_params = grid_search.best_params_\n",
    "#     # Test Results\n",
    "#     class_report = sklearn.metrics.classification_report(Y_test, prediction)\n",
    "#     print('Best Parameters (test) =>', best_params, '\\n','\\n')\n",
    "#     print('Train Classification Report (test): ', '\\n', class_report)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris.Cirelli\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mix type of y not allowed, got types {'multiclass', 'multilabel-indicator'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-4fa86b73cc06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtree_I_GridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-a403fc2a7ded>\u001b[0m in \u001b[0;36mtree_I_GridSearchCV\u001b[1;34m(Features, Targets)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mbest_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Train Results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mclass_report\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best Parameters (train) =>'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train Classification Report (train): '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_report\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Chris.Cirelli\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits)\u001b[0m\n\u001b[0;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1390\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1391\u001b[1;33m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1392\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1393\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Chris.Cirelli\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mys_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mix type of y not allowed, got types %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mys_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mlabel_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mys_types\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Mix type of y not allowed, got types {'multiclass', 'multilabel-indicator'}"
     ]
    }
   ],
   "source": [
    "tree_I_GridSearchCV(df_feature, df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Best Fit to Recall - Issue.  it is fitting recall very high for the non-claims, but still very low for teh actual claims.  See if you can run this for precision to get a better score. '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#    Test GridSearchCV Tree on test subjects\n",
    "def tree_I_best_fit(Features, Targets):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Features, Targets, test_size=0.3, random_state = 100, stratify = Targets)\n",
    "    tree = DecisionTreeClassifier(max_depth= 4, \n",
    "                                  max_features= 4, \n",
    "                                  max_leaf_nodes = 20, \n",
    "                                  min_samples_leaf= 25, \n",
    "                                  min_samples_split = 35, \n",
    "                                  min_weight_fraction_leaf = 0.01)\n",
    "    tree.fit(X_train, y_train)   \n",
    "    \n",
    "    #prediction_train = tree.predict(X_train)\n",
    "    #class_report_train = sklearn.metrics.classification_report(y_train, prediction_train)\n",
    "    #print(class_report_train)\n",
    "    prediction_test = tree.predict(X_test)\n",
    "    class_report_test = sklearn.metrics.classification_report(y_test, prediction_test)\n",
    "    print(class_report_test)\n",
    "\n",
    "'''Best Fit to Recall - Issue.  it is fitting recall very high for the non-claims, but still very low for teh actual claims.  See if you can run this for precision to get a better score. '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.84      0.50      0.63        32\n",
      "          2       0.74      0.47      0.58        36\n",
      "          3       0.00      0.00      0.00        30\n",
      "          4       0.00      0.00      0.00        17\n",
      "          5       0.51      1.00      0.68       178\n",
      "          6       0.00      0.00      0.00        22\n",
      "          7       0.00      0.00      0.00        22\n",
      "          8       0.00      0.00      0.00        24\n",
      "          9       0.00      0.00      0.00         1\n",
      "         10       0.00      0.00      0.00        11\n",
      "         11       0.00      0.00      0.00        16\n",
      "\n",
      "avg / total       0.37      0.54      0.42       389\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris.Cirelli\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "tree_I_best_fit(df_feature, df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#   Original Random Tree\n",
    "def random_tree_I(Features, Targets):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Features, Targets, test_size = 0.3, random_state = 50)\n",
    "    forest = RandomForestClassifier(n_estimators = 100, max_depth = 55, min_samples_split = 2, max_features = 4, min_samples_leaf = 1, min_impurity_split = .03)\n",
    "    forest.fit(X_train, y_train)\n",
    "    prediction_train = forest.predict(X_train)\n",
    "    recall_score_train = sklearn.metrics.classification_report(y_train, prediction_train)\n",
    "    prediction_test = forest.predict(X_test)\n",
    "    recall_score_test = sklearn.metrics.classification_report(y_test, prediction_test)\n",
    "    print('Recall score train => ', recall_score_train)\n",
    "    print('')\n",
    "    print('Recall score test => ', recall_score_test)\n",
    "        \n",
    "#   Best fit Random Tree\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score train =>               precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00        76\n",
      "          2       1.00      0.99      0.99        81\n",
      "          3       1.00      0.97      0.98        65\n",
      "          4       0.92      0.95      0.93        37\n",
      "          5       1.00      1.00      1.00       423\n",
      "          6       1.00      1.00      1.00        51\n",
      "          7       0.98      0.96      0.97        51\n",
      "          8       0.93      0.96      0.94        52\n",
      "          9       1.00      1.00      1.00         2\n",
      "         10       1.00      1.00      1.00        29\n",
      "         11       1.00      1.00      1.00        39\n",
      "\n",
      "avg / total       0.99      0.99      0.99       906\n",
      "\n",
      "\n",
      "Recall score test =>               precision    recall  f1-score   support\n",
      "\n",
      "          1       0.97      0.91      0.94        32\n",
      "          2       0.94      0.89      0.92        38\n",
      "          3       0.85      0.65      0.73        34\n",
      "          4       0.29      0.33      0.31        21\n",
      "          5       0.94      0.98      0.96       171\n",
      "          6       0.83      0.91      0.87        22\n",
      "          7       0.79      0.83      0.81        23\n",
      "          8       0.52      0.56      0.54        27\n",
      "         10       1.00      0.75      0.86         8\n",
      "         11       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.86      0.85      0.85       389\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_tree_I(df_feature, df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Example of Bagging\n",
    "    \n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "#from sklear.tree import DecisionTreeClassifier\n",
    "\n",
    "def decision_tree_baggin(Features, Targets):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Features, Targets, test_size = 0.3)\n",
    "    bag_clf = BaggingClassifier(\n",
    "            n_estimators = 5,                      #train 500 different Decision Tree Classifiers. \n",
    "            max_samples = 200, \n",
    "            bootstrap = True, \n",
    "            n_jobs = -1)                            #number of classifiers to run at the same time.  If -1, then num jobs = num cores. \n",
    "\n",
    "    bag_clf.fit(X_train, y_train)\n",
    "    y_pred = bag_clf.predict(X_test)\n",
    "    score = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "    print(score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bagger(Features, Targets):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Features, Targets)\n",
    "    bg = BaggingClassifier(\n",
    "            RandomForestClassifier(),\n",
    "            max_samples = 2000, \n",
    "            max_features = 3, \n",
    "            n_estimators = 100)\n",
    "    bg.fit(X_train, y_train)\n",
    "    prediction = bg.predict(X_test)\n",
    "    score = sklearn.metrics.classification_report(y_test, prediction)\n",
    "    print(score)\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Booster(Features, Targets):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Features, Targets)    \n",
    "    ada_clf = AdaBoostClassifier(\n",
    "            DecisionTreeClassifier(max_depth= 8, \n",
    "                                  max_features= 4, \n",
    "                                  max_leaf_nodes = 20, \n",
    "                                  min_samples_leaf= 25, \n",
    "                                  min_samples_split = 35, \n",
    "                                  min_weight_fraction_leaf = 0.01),\n",
    "            n_estimators = 200)\n",
    "    ada_clf.fit(X_train, y_train)\n",
    "    #prediction = ada_clf.predict((X_train))\n",
    "    #score = sklearn.metrics.classification_report(y_train, prediction)\n",
    "    prediction = ada_clf.predict((X_test))\n",
    "    score = sklearn.metrics.classification_report(y_test, prediction)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.40      0.79      0.53        28\n",
      "          2       0.64      0.29      0.40        31\n",
      "          3       0.54      0.54      0.54        26\n",
      "          4       0.33      0.33      0.33        12\n",
      "          5       0.84      0.78      0.81       150\n",
      "          6       0.71      0.56      0.63        18\n",
      "          7       0.17      0.26      0.20        19\n",
      "          8       0.19      0.18      0.18        17\n",
      "         10       1.00      0.44      0.62         9\n",
      "         11       1.00      1.00      1.00        14\n",
      "\n",
      "avg / total       0.67      0.62      0.63       324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Booster(df_feature, df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTreeRegressor_Boost(Features, Targets):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Features, Targets)  \n",
    "    tree_reg1 = DecisionTreeRegressor()\n",
    "    tree_reg1.fit(X_train, y_train)   \n",
    "    prediction = tree_reg1.predict(X_train)\n",
    "    report = sklearn.metrics.classification_report(y_train, prediction)\n",
    "    return report\n",
    "     \n",
    "report = DecisionTreeRegressor(df_feature, df_target)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
