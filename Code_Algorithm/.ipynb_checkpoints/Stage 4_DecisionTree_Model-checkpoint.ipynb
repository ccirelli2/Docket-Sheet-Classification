{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import graphviz\n",
    "from sklearn.tree import export_graphviz    \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#   Classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\Chris.Cirelli\\\\Desktop\\\\Python Programming Docs\\\\GSU\\\\Sprint Project\\\\Docket-Sheet-Classification\\\\Modules')\n",
    "import Step1_Module as stp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\\\Users\\\\Chris.Cirelli\\\\Desktop\\\\Python Programming Docs\\\\GSU\\\\Sprint Project\\\\Docket-Sheet-Classification\\\\Result_Files_Key_Word_Attempt_2')\n",
    "df_DAT = pd.read_excel(r'Data Analytics Table_WordMatch_All_Docket_Entries_v4_03.10.2018.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Index (Docket Sheet Entries)\n",
    "\n",
    "df_reset_index = df_DAT.reset_index()\n",
    "df_drop_col1 = df_reset_index.drop('index', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = df_drop_col1.drop('Life Cycle Stage', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = df_drop_col1['Life Cycle Stage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Simple Decision Tree\n",
    "\n",
    "def simple_decision_tree(Features, Targets, max_depth):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Features, Targets, test_size=0.2)\n",
    "    clf = DecisionTreeClassifier(max_depth = 8, random_state = 50)\n",
    "    \n",
    "    # Fit Algorithm\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    #   Train\n",
    "    clf_pred = clf.predict(X_train)\n",
    "    report_train = sklearn.metrics.classification_report(y_train, clf_pred)\n",
    "    matrix_train = sklearn.metrics.confusion_matrix(y_train, clf_pred)\n",
    "    \n",
    "    #   Test\n",
    "    clf_pred = clf.predict(X_test)\n",
    "    report_test = sklearn.metrics.classification_report(y_test, clf_pred)\n",
    "    matrix_test = sklearn.metrics.confusion_matrix(y_test, clf_pred)\n",
    "    \n",
    "    print('Report (Train)', '\\n', report_train, '\\n')\n",
    "    print('Report (Test)', '\\n', report_test, '\\n')\n",
    "    print('Confusion Matrix (Test)', '\\n', matrix_test, '\\n')\n",
    "    \n",
    "    return matrix_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report (Train) \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      0.72      0.84        90\n",
      "          2       1.00      0.88      0.94        93\n",
      "          3       0.58      0.35      0.44        74\n",
      "          4       0.48      0.27      0.34        49\n",
      "          5       0.76      0.99      0.86       476\n",
      "          6       0.87      0.93      0.90        58\n",
      "          7       0.73      0.76      0.75        62\n",
      "          8       1.00      0.03      0.06        63\n",
      "         10       0.96      0.93      0.95        29\n",
      "         11       1.00      0.98      0.99        42\n",
      "\n",
      "avg / total       0.81      0.80      0.77      1036\n",
      " \n",
      "\n",
      "Report (Test) \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      0.56      0.71        18\n",
      "          2       0.88      0.88      0.88        26\n",
      "          3       0.54      0.28      0.37        25\n",
      "          4       0.25      0.11      0.15         9\n",
      "          5       0.75      0.99      0.85       118\n",
      "          6       0.79      1.00      0.88        15\n",
      "          7       0.67      0.67      0.67        12\n",
      "          8       0.00      0.00      0.00        16\n",
      "          9       0.00      0.00      0.00         2\n",
      "         10       1.00      1.00      1.00         8\n",
      "         11       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       0.70      0.77      0.72       259\n",
      " \n",
      "\n",
      "Confusion Matrix (Test) \n",
      " [[ 10   0   0   0   8   0   0   0   0   0   0]\n",
      " [  0  23   0   0   1   0   2   0   0   0   0]\n",
      " [  0   3   7   0  13   2   0   0   0   0   0]\n",
      " [  0   0   1   1   5   0   2   0   0   0   0]\n",
      " [  0   0   1   0 117   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0  15   0   0   0   0   0]\n",
      " [  0   0   0   0   2   2   8   0   0   0   0]\n",
      " [  0   0   4   3   9   0   0   0   0   0   0]\n",
      " [  0   0   0   0   2   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   8   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  10]] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris.Cirelli\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "test = simple_decision_tree(df_feature, df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
